{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, time)\n",
    "        attn = torch.sigmoid(self.conv(x))  # (batch, 1, time)\n",
    "        return x * attn\n",
    "\n",
    "class LocalSpikeDetectionBranch(nn.Module):\n",
    "    def __init__(self, kernel_size, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=0,  # 'valid' padding to prevent boundary effects\n",
    "            bias=True\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class EpilepsyDetectionModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels=32,\n",
    "        sampling_rate=300,  # Hz\n",
    "        num_classes=2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Calculate kernel sizes based on sampling rate\n",
    "        # For 70ms, 150ms, and 200ms spikes\n",
    "        self.kernel_sizes = [\n",
    "            int(0.07 * sampling_rate),  # 70ms\n",
    "            int(0.15 * sampling_rate),  # 150ms\n",
    "            int(0.20 * sampling_rate)   # 200ms\n",
    "        ]\n",
    "        \n",
    "        # Local Feature Detection Branches\n",
    "        self.branches = nn.ModuleList([\n",
    "            LocalSpikeDetectionBranch(\n",
    "                kernel_size=k,\n",
    "                in_channels=num_channels,\n",
    "                out_channels=128\n",
    "            ) for k in self.kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Calculate the size of concatenated features\n",
    "        # Need to account for the length reduction from valid convolutions\n",
    "        self.feature_size = self._calculate_feature_size(1000, num_channels)\n",
    "        \n",
    "        # Spatial Integration\n",
    "        self.channel_conv = nn.Conv1d(384, 256, kernel_size=1)\n",
    "        self.spatial_attention = SpatialAttention(256)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.fc1 = nn.Linear(self.feature_size, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv1d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    def _calculate_feature_size(self, input_length, num_channels):\n",
    "        # Helper function to calculate feature size after convolutions and pooling\n",
    "        x = torch.randn(1, num_channels, input_length)\n",
    "        branch_outputs = []\n",
    "        \n",
    "        for branch in self.branches:\n",
    "            branch_outputs.append(branch(x))\n",
    "            \n",
    "        x = torch.cat(branch_outputs, dim=1)\n",
    "        x = self.channel_conv(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x.numel() // x.shape[0]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, time)\n",
    "        \n",
    "        # Process each branch\n",
    "        branch_outputs = []\n",
    "        for branch in self.branches:\n",
    "            branch_outputs.append(branch(x))\n",
    "            \n",
    "        # Concatenate branch outputs\n",
    "        x = torch.cat(branch_outputs, dim=1)  # (batch, 384, time)\n",
    "        \n",
    "        # Spatial integration\n",
    "        x = self.channel_conv(x)  # (batch, 256, time)\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Flatten for classification\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classification\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training utilities\n",
    "class EpilepsyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "class SpikeAugmentation:\n",
    "    def __init__(self, sigma=0.01, time_shift=15, scale_range=(0.9, 1.1)):\n",
    "        self.sigma = sigma\n",
    "        self.time_shift = time_shift\n",
    "        self.scale_range = scale_range\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # Add Gaussian noise\n",
    "        x = x + torch.randn_like(x) * self.sigma\n",
    "        \n",
    "        # Random time shift\n",
    "        if self.time_shift > 0:\n",
    "            shift = torch.randint(-self.time_shift, self.time_shift + 1, (1,))\n",
    "            x = torch.roll(x, shifts=shift.item(), dims=-1)\n",
    "            \n",
    "        # Random scaling\n",
    "        if self.scale_range[0] < self.scale_range[1]:\n",
    "            scale = torch.empty(1).uniform_(*self.scale_range)\n",
    "            x = x * scale\n",
    "            \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "def train_model():\n",
    "    # Hyperparameters\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = EpilepsyDetectionModel().to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    transform = SpikeAugmentation()\n",
    "    train_dataset = EpilepsyDataset(train_data, train_labels, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
