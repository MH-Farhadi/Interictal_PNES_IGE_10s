{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For the GCN layers\n",
    "# pip install torch-geometric\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "\n",
    "class EEGEpilepsyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A hybrid 1D CNN + GCN model to extract epileptic features from EEG data.\n",
    "\n",
    "    Input shape:\n",
    "      x: (batch_size, num_channels=21, num_samples=6300)\n",
    "      edge_index: (2, E) - graph connectivity for the 21 channels\n",
    "\n",
    "    Steps:\n",
    "      1) For each channel (21 total):\n",
    "         - pass its 1D time series (6300 samples) through CNN layers\n",
    "         - obtain a channel-wise feature vector\n",
    "      2) Stack these feature vectors to get a node feature matrix of shape (21, feature_dim)\n",
    "      3) Pass that to a GCN (using the provided edge_index) to learn spatial/functional relationships\n",
    "      4) Optionally pool or reduce over nodes to get a graph-level embedding\n",
    "      5) Classify via fully connected layer\n",
    "\n",
    "    Arguments:\n",
    "      - adjacency (torch.Tensor): 21x21 adjacency matrix or None\n",
    "        (Used here only if you prefer to construct edge_index inside the model)\n",
    "      - in_channels (int): Number of input channels to CNN (for EEG time series it's typically 1)\n",
    "      - cnn_features (int): Output feature dimension from the CNN for each channel\n",
    "      - hidden_dim (int): Hidden dimension for GCN\n",
    "      - num_classes (int): Number of output classes\n",
    "      - kernel_size (int): Kernel size for CNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adjacency: torch.Tensor = None,\n",
    "        in_channels: int = 1,\n",
    "        cnn_features: int = 16,\n",
    "        hidden_dim: int = 32,\n",
    "        num_classes: int = 2,\n",
    "        kernel_size: int = 3\n",
    "    ):\n",
    "        super(EEGEpilepsyNet, self).__init__()\n",
    "\n",
    "        self.num_eeg_channels = 21\n",
    "\n",
    "        # If an adjacency (21 x 21) is provided, convert to edge_index once here\n",
    "        if adjacency is not None:\n",
    "            edge_index, _ = dense_to_sparse(adjacency)\n",
    "        else:\n",
    "            # If no adjacency is provided, you must pass edge_index each forward pass\n",
    "            edge_index = None\n",
    "        self.register_buffer(\"edge_index\", edge_index)\n",
    "\n",
    "        # Example 1D CNN layers for each channel\n",
    "        # You can stack more layers or use larger filters if needed.\n",
    "        self.conv1 = nn.Conv1d(in_channels, 8, kernel_size=kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(8, cnn_features, kernel_size=kernel_size, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Two-layer GCN\n",
    "        self.gcn1 = GCNConv(cnn_features, hidden_dim)\n",
    "        self.gcn2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: EEG input of shape (batch_size, 21, 6300)\n",
    "            edge_index: (2, E) - optional if not stored in self.edge_index\n",
    "        Returns:\n",
    "            logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # If the model has a built-in edge_index, we can fall back on it if not provided\n",
    "        if edge_index is None:\n",
    "            edge_index = self.edge_index\n",
    "            if edge_index is None:\n",
    "                raise ValueError(\n",
    "                    \"No edge_index found. Please provide adjacency or edge_index.\"\n",
    "                )\n",
    "\n",
    "        # 1) CNN feature extraction per channel\n",
    "        channel_features = []\n",
    "        for ch in range(self.num_eeg_channels):\n",
    "            # Extract single channel: shape (batch_size, 6300)\n",
    "            single_channel = x[:, ch, :].unsqueeze(1)  # -> (batch_size, 1, 6300)\n",
    "            out = F.relu(self.conv1(single_channel))   # -> (batch_size, 8, 6300)\n",
    "            out = F.relu(self.conv2(out))              # -> (batch_size, cnn_features, 6300)\n",
    "            out = self.pool(out)                       # -> (batch_size, cnn_features, 1)\n",
    "            out = out.squeeze(-1)                      # -> (batch_size, cnn_features)\n",
    "            channel_features.append(out)\n",
    "\n",
    "        # Stack channel-wise features into shape: (batch_size, 21, cnn_features)\n",
    "        channel_features = torch.stack(channel_features, dim=1)\n",
    "\n",
    "        # 2) GCN for graph-based feature extraction\n",
    "        # PyG expects node features as (num_nodes, num_features).\n",
    "        # We have them in (batch_size, 21, cnn_features).\n",
    "        # We'll process each sample in the batch individually, then combine results.\n",
    "\n",
    "        graph_embeddings = []\n",
    "        for b in range(batch_size):\n",
    "            node_feats = channel_features[b]  # -> (21, cnn_features)\n",
    "\n",
    "            # Pass node_feats through GCN\n",
    "            g = self.gcn1(node_feats, edge_index)  # -> (21, hidden_dim)\n",
    "            g = F.relu(g)\n",
    "            g = self.gcn2(g, edge_index)           # -> (21, hidden_dim)\n",
    "            g = F.relu(g)\n",
    "\n",
    "            # 3) Pool the node embeddings to get a graph-level embedding\n",
    "            # A simple approach: average over the 21 nodes\n",
    "            g = g.mean(dim=0)  # -> (hidden_dim,)\n",
    "\n",
    "            graph_embeddings.append(g)\n",
    "\n",
    "        # Stack each graph embedding in batch: (batch_size, hidden_dim)\n",
    "        graph_embeddings = torch.stack(graph_embeddings, dim=0)\n",
    "\n",
    "        # 4) Classification\n",
    "        logits = self.fc(graph_embeddings)  # -> (batch_size, num_classes)\n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Example Usage\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # Suppose we have a random adjacency for 21 channels (21 x 21).\n",
    "    # In practice, you'd build this from known sensor layout or correlation metrics, etc.\n",
    "    adjacency = torch.randint(0, 2, (21, 21)).float()\n",
    "    # Make adjacency symmetric\n",
    "    adjacency = (adjacency + adjacency.t()) / 2\n",
    "    adjacency[adjacency > 0] = 1\n",
    "\n",
    "    # Create the network\n",
    "    model = EEGEpilepsyNet(adjacency=adjacency, num_classes=2)\n",
    "\n",
    "    # Simulate a batch of EEG data\n",
    "    # batch_size = 4, 21 channels, 6300 samples per channel\n",
    "    x = torch.randn(4, 21, 6300)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(x)  # shape: (4, 2)\n",
    "    print(\"Output logits shape:\", logits.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
